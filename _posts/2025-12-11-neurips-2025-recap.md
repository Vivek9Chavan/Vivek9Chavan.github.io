---
layout: post
title: "NeurIPS 2025: Reflections, Highlights, and Key Takeaways"
date: 2025-12-11
---

### TL;DR
*   **The Scale:** We hit a breaking point. **21,575 submissions** (+61% from last year) and over 20,000 reviewers. The conference is now the size of a small city.
*   **The Shift:** The "Chatbot" era is ending; the **"Reasoning & Agents"** era has begun. The focus has moved from *what models know* to *how they think*.
*   **The Vibe:** Optimism mixed with "Data Anxiety." Everyone is excited about System 2 thinking, but nervous about where the next trillion tokens will come from.

---

<div style="display: flex; align-items: center; gap: 20px; margin-top: 1.5em; margin-bottom: 1.5em;">

  <!-- Image column (50%) -->
  <div style="flex: 0 0 50%;">
    <img src="/assets/IMG_0628.jpeg"
         alt="Author at the San Diego Convention Center during NeurIPS 2025"
         style="width: 100%; border-radius: 6px;">
  </div>

  <!-- Text column (50%) -->
  <div style="flex: 0 0 50%; text-align: left;">
    <p><strong>Reflecting on a week of ideas, conversations, and shared enthusiasm.</strong></p>

    <p>
      I just returned from San Diego after attending the NeurIPS conference last week. It was an amazing experience, though the sheer amount of information and activity was substantial. I took some time to reflect on everything I saw. This post summarizes the most important and noteworthy takeaways from my experience.
    </p>
  </div>

</div>



## Trends and Observations

The numbers defining NeurIPS 2025 are significant. There were 21,575 submissions, which represents a 61% increase over 2024. To put that in perspective, there were only about 9,400 submissions in 2020, meaning the field has more than doubled in five years. Despite the flood of papers, the acceptance rate remained steady at 24.5%, resulting in 5,290 accepted papers. To handle this volume, the organisers mobilised 20,518 reviewers. The peer-review process now requires a population larger than the seating capacity of Madison Square Garden.

The scale of the event was equally impressive in terms of attendance. There were roughly 29,000 total registrations. It is estimated that between 22,000 and 25,000 people attended in person in San Diego. This likely makes it the biggest academic gathering in history. The event was also diverse in its structure. In addition to the main algorithmic track, there were tracks for datasets and benchmarks, creative AI, position papers, and journals. This was all part of the main conference, which ran alongside workshops and other affinity events.

My personal observations matched these statistics. Walking through the hallways and listening to conversations, I frequently heard specific keywords. The most common terms were scale, reinforcement learning (RL), language models, world models, reasoning, and compute. These themes were reflected in the keynotes and poster sessions. The general trend holds that research based on Large Language Models (LLMs) dominated the conference and remains the prevailing direction for the AI community at large.


## Keynotes and Invited Talks

The conference featured six invited talks and one presentation for the Test of Time Award. I focused my attention on three specific sessions that offered distinct perspectives on the future of the field.

### The OAK Architecture: A Vision of Superintelligence from Experience
**Speaker:** Rich Sutton

Rich Sutton presented his perspective on the current state of AI research through the lens of the OAK architecture. His central thesis aligned closely with his well-known 'Bitter Lesson' hypothesis. He argued that to achieve true superintelligence, we must allow agents to learn entirely from experience. He cautioned against trying to explicitly induce user bias or expert knowledge into these systems.

The most surprising moment of the talk was his stance on continual learning. Sutton believes that robust and reliable continual learning is absolutely essential for superintelligence. While we do not possess this capability yet, he predicted that we might solve it within the next one to two years. This was a bold statement regarding a problem that remains an open question in the community. His belief suggests that we could arrive at superintelligence much sooner than many expect.

<div style="display: flex; gap: 12px; margin-bottom: 0.5em;">

  <div style="flex: 1;">
    <img src="/assets/IMG_0057.jpeg" alt="Richard Sutton keynote NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

  <div style="flex: 1;">
    <img src="/assets/IMG_0062.jpeg" alt="Opening keynote hall at NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

</div>

<p style="text-align: center; font-style: italic; margin-top: -0.2em;">
  The keynote sessions were organised in the giant exhibit hall (left). Opening keynote by Richard Sutton, delivered to a large audience at NeurIPS 2025.
</p>

### Are We Having the Wrong Nightmares About AI?
**Speaker:** Zeynep Tufekci

This talk moved away from technical architecture to address the societal impact of the technology. Tufekci argued that we are focusing on the wrong fears. Instead of worrying about AGI killing everyone or taking all jobs, she highlighted the realistic implications we are already seeing. She described the current generation of models as 'Artificial Good-Enough Intelligence'. These systems are capable enough to shatter the mechanisms of proof and authenticity that ground our social order. She implored AI researchers to put ethics and the impact of their work front and centre, rather than prioritising profits and high salaries.

### On the Science of “Alien Intelligences”
**Speaker:** Melanie Mitchell

Melanie Mitchell offered a compelling analogy for understanding Large Language Models (LLMs) and Vision Language Models (VLMs). She suggested we imagine an alien species has landed on Earth. This species appears intelligent, but the way it processes information is vastly different from human cognition. She argued that this is exactly how we should view AI. These models possess interesting capabilities, but we often draw false correlations regarding their understanding of the world. She recommended applying experimental protocols from developmental psychology to better evaluate these non-human cognitive systems.

### Other Invited Talks

I was unable to attend the full sessions for the remaining three keynotes, but they covered significant themes regarding the limits and theory of deep learning.

*   **The Art of (Artificial) Reasoning (Yejin Choi):** Choi discussed the limitations of brute-force scaling. She highlighted that state-of-the-art models still exhibit 'jagged intelligence' and argued for enhancing reasoning capabilities in smaller models to improve sustainability.
*   **From Benchmarks to Problems (Kyunghyun Cho):** Cho provided a retrospective on his work across distinct fields, from machine translation to drug discovery. He focused on the importance of problem-finding in AI and how seemingly unrelated technical problems often share close connections.
*   **Demystifying Depth (Andrew Saxe):** Saxe presented mathematical analyses of the nonlinear dynamics in deep neural networks. His talk offered theoretical insights into how learning algorithms and architectural choices interact to produce complex generalisation behaviours.

### General Inferences

The selection of these speakers signals a clear intention by the conference organisers. They placed the pursuit of autonomous, scaling-based intelligence (Sutton) directly alongside the need for caution and rigorous evaluation (Mitchell, Choi, Tufekci). It is evident that while the community is pushing for more powerful systems, there is a growing recognition that we must better understand the alien intelligence we are building before we integrate it further into society.


## Best Paper Awards

The Best Paper Award committees were tasked with selecting highly impactful papers from both the Main Track and the Datasets & Benchmark Track. Nominations were made by Program Chairs and approved by General Chairs, resulting in seven groundbreaking papers being recognised this year.

### Outstanding Paper Award
**Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free**
*Zihan Qiu et al.*

This paper addresses a fundamental component of modern AI: the attention mechanism. The authors conducted comprehensive experiments on over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models. Their central finding is that a simple modification—applying a head-specific sigmoid gate after the Scaled Dot-Product Attention—consistently improves performance. This "gated attention" enhances training stability, allows for larger learning rates, and mitigates the "attention sink" phenomenon. The selection committee praised the work for its extensive empirical evidence and for openly sharing results from industrial-scale experiments, a practice that has become increasingly rare.

### Outstanding Paper Award (Datasets & Benchmarks Track)
**Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)**
*Liwei Jiang et al.*

This paper tackles the issue of diversity in AI-generated content. The authors introduced "Infinity-Chat," a dataset of 26,000 open-ended user queries, to study how models respond to creative prompts. They discovered a "Hivemind" effect where models not only repeat themselves (intra-model repetition) but also produce strikingly similar outputs to one another (inter-model homogeneity). This work highlights a critical miscalibration between current reward models and diverse human preferences, raising concerns about the long-term homogenisation of human thought.

### Outstanding Paper Award
**1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities**
*Kevin Wang et al.*

While scaling has revolutionised language and vision, Reinforcement Learning (RL) has largely relied on shallow networks (2–5 layers). This paper challenges that convention by demonstrating that increasing depth to 1,024 layers can significantly boost performance in self-supervised RL. The authors show that deep networks allow agents to learn complex goal-reaching behaviours from scratch without external rewards. This finding suggests that RL can indeed scale efficiently with depth, unlocking new capabilities previously thought impossible without heavy supervision.

### Outstanding Paper Award
**Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training**
*Tony Bonnaire et al.*

This theoretical work investigates why diffusion models are able to generalise rather than simply memorising their training data. The authors identified two distinct timescales in training: an early phase for generalisation and a later phase where memorisation begins. Crucially, they found that the window for effective generalisation grows linearly with the size of the training set. This provides a rigorous mathematical explanation for the success of diffusion models, linking their performance to implicit dynamical regularisation.

### Test of Time Award
**Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks**
*Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun*

<div style="display: flex; align-items: center; gap: 20px; margin-bottom: 1.5em;">

  <!-- Image column (50%) -->
  <div style="flex: 0 0 50%;">
    <img src="/assets/IMG_0134.jpeg" alt="Kaiming He delivering the Test of Time keynote at NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

  <!-- Caption column (50%) -->
  <div style="flex: 0 0 50%; font-style: italic; text-align: left;">
    <p><strong>Kaiming He delivering the Test of Time keynote.</strong></p>
    <p>The Test of Time lecture highlighted a decade of influence from foundational work in computer vision and deep learning, marking how these early ideas continue to shape modern AI research.</p>
  </div>

</div>

The Test of Time Award was given to the seminal 2015 paper *Faster R-CNN*, which revolutionised object detection. The acceptance speech was delivered by Kaiming He and was one of the most attended events of the conference.

He began with a general overview of the computer vision landscape before and after AlexNet, tracing the shift towards deep architectures that motivated this work. He then took a philosophical turn, describing his experience over the last decade as that of an explorer in uncharted waters. He likened research to searching for unknown lands, never knowing if discovery is around the corner or miles away. He remained remarkably humble, noting that newer innovations—such as the *Segment Anything Model*—would eventually make works like *Faster R-CNN* obsolete. His talk served as a realistic and inspiring impetus for continued exploration in the field.

### Sejnowski-Hinton Prize
**Feedback Alignment**
*Timothy Lillicrap et al.*

The 2025 Sejnowski-Hinton Prize was awarded to the 2016 paper *Random synaptic feedback weights support error backpropagation for deep learning*. The authors discovered "feedback alignment," demonstrating that neural networks can learn effectively using fixed, random feedback weights rather than the exact symmetry required by backpropagation. This provided a biologically plausible solution to the "weight transport problem," showing how real neurons might follow loss gradients without non-local information.

## Poster Presentations: The Real Conference

If the keynotes are the "concert," the poster sessions are the "mosh pit."

This is where the real work of NeurIPS happens. The setup was the usual endless maze of boards, but the crowd dynamics were fascinating this year.

<div style="margin-bottom: 1em;">
  <img src="/assets/IMG_0112.jpeg" alt="Poster session at NeurIPS 2025" style="width: 100%; border-radius: 6px;">
</div>

<p style="text-align: center; font-style: italic; margin-top: -0.5em;">
  Lively and crowded poster sessions throughout the week.
</p>

### The "Hype Circles"
You could instantly tell which papers were trending on Twitter (X) by the crowd density.
*   **The "Reasoning" Crowd:** Any poster with "System 2," "Chain of Thought," or "O1-like" in the title had a crowd five people deep. I tried to get close to a poster about **"Recursive Self-Correction in Agents,"** but I honestly couldn't hear the presenter over the questions.
*   **The "Theory" Deserts:** In contrast, the pure theory sections (optimization bounds, convex analysis) were quieter. However, this is where I had my best conversation of the week. I spent twenty minutes talking to a PhD student about the **theoretical limits of in-context learning**. No hype, no buzzwords, just math. It was a breath of fresh air.

### My Personal Favorite Poster
I stumbled upon a paper regarding **"Small Language Models as Verifiers."**
The premise was simple: you don't need a massive model to *generate* the answer, but you can use a massive model to *check* the work of a smaller model. It felt like a practical, immediate solution to deployment costs that I can actually use in my work next week.


<div style="display: flex; gap: 12px; margin-bottom: 0.5em;">

  <div style="flex: 1;">
    <img src="/assets/IMG_0068.jpeg" alt="Job board at the start of NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

  <div style="flex: 1;">
    <img src="/assets/IMG_0172.jpeg" alt="Job board by day three at NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

</div>

<p style="text-align: center; font-style: italic; margin-top: -0.2em;">
  The job board at the beginning of the conference (left) and by the end of day 3 (right).
</p>

---

## The Expo: Where the Money Is

Walking from the poster hall to the Expo Hall felt like crossing a border between two different countries.

*   **The Poster Hall:** Academic, messy, poorly dressed, fueled by bad coffee.
*   **The Expo Hall:** Corporate, polished, aggressive, fueled by venture capital.

The Expo is where the sponsors flex. The booths this year were larger than ever, with NVIDIA, Google DeepMind, and OpenAI taking up massive real estate.

<div style="display: flex; gap: 12px; justify-content: center; align-items: flex-start; margin-bottom: 0.5em;">

  <!-- Landscape image (will be wider naturally) -->
  <img src="/assets/IMG_0064.jpeg"
       alt="Expo booth at NeurIPS 2025"
       style="height: 280px; width: auto; border-radius: 6px;">

  <!-- Portrait image (will be narrower naturally) -->
  <img src="/assets/IMG_0090.jpeg"
       alt="Expo demonstration at NeurIPS 2025"
       style="height: 280px; width: auto; border-radius: 6px;">

</div>

<p style="text-align: center; font-style: italic; margin-top: -0.2em;">
  Scenes from this year’s NeurIPS Expo.
</p>

### What I Noticed:
1.  **The "Full Stack" Pivot:** In previous years, companies were selling "Model APIs." This year, everyone was selling **"Enterprise Platforms."** The pitch has shifted from "Use our LLM" to "Build your entire Agentic Workflow on our secure infrastructure."
2.  **Robotics is Back:** There were robot dogs, humanoid arms folding laundry, and drones everywhere. With the rise of multimodal models (vision + language + action), robotics has finally found the "brain" it was missing for the last decade.
3.  **Recruiting Wars:** The swag was better this year, which usually means the recruiting is desperate. I saw companies handing out high-end mechanical keyboards just to get a resume scan.

Despite the flashiness, I found the Expo a bit exhausting. It’s a stark reminder that while the researchers upstairs are trying to solve AGI, the downstairs crowd is trying to figure out how to sell it to the Fortune 500.

---

## Workshops:

<div style="display: flex; align-items: center; gap: 20px; margin-bottom: 1.5em;">

  <!-- Image column (50%) -->
  <div style="flex: 0 0 50%;">
    <img src="/assets/IMG_0342.jpeg" alt="Chelsea Finn speaking at the Embodied World Models for Decision Making workshop at NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

  <!-- Caption column (50%) -->
  <div style="flex: 0 0 50%; font-style: italic; text-align: left;">
    <p><strong>Chelsea Finn speaking at the “Embodied World Models for Decision Making” workshop.</strong></p>
    <p>The workshop sessions on days 6 and 7 brought a more intimate, research-focused atmosphere, with deep discussions on embodied agents, world models, and decision-making across interactive environments.</p>
  </div>

</div>

---

## General

<div style="display: flex; gap: 12px; margin-bottom: 0.5em;">

  <div style="flex: 1;">
    <img src="/assets/IMG_0572.jpeg" alt="Gaslamp Quarter view from NeurIPS venue" style="width: 100%; border-radius: 6px;">
  </div>

  <div style="flex: 1;">
    <img src="/assets/IMG_0435.jpeg" alt="San Diego Bay and Coronado view from NeurIPS venue" style="width: 100%; border-radius: 6px;">
  </div>

</div>

<p style="text-align: center; font-style: italic; margin-top: -0.2em;">
  Views from outside the conference venue. Many participants could be seen socialising, taking a stroll, or relaxing here during breaks and evenings.
</p>
