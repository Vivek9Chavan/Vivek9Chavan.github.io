---
layout: post
title: "NeurIPS 2025: Reflections, Highlights, and Key Takeaways"
date: 2025-12-11
---

### TL;DR

- **Unprecedented Scale:** NeurIPS has evolved from an academic conference into a massive global event, with attendance and submissions breaking all previous records.
- **The Big Shift:** The research focus is moving beyond standard LLMs toward reasoning capabilities, agentic workflows, and world models. The goal is no longer just generating text, but enabling AI to plan and act.
- **Key Debates:** The invited talks highlighted a tension between the pursuit of autonomous superintelligence (via Reinforcement Learning) and the urgent need to understand the "alien" nature of these systems before integrating them deeper into society.
- **The Experience:** Despite the overwhelming size and heavy corporate presence, the core value remains in the poster sessions and face-to-face networking with the research community.

---

<div style="display: flex; align-items: center; gap: 20px; margin-top: 1.5em; margin-bottom: 1.5em;">

  <!-- Image column (50%) -->
  <div style="flex: 0 0 50%;">
    <img src="/assets/IMG_0628.jpeg"
         alt="Author at the San Diego Convention Center during NeurIPS 2025"
         style="width: 100%; border-radius: 6px;">
  </div>

  <!-- Text column (50%) -->
  <div style="flex: 0 0 50%; text-align: left;">
    <p><strong>Reflecting on a week of ideas, conversations, and shared enthusiasm.</strong></p>

    <p>
      I just returned from San Diego after attending the NeurIPS conference last week. It was an amazing experience, though the sheer amount of information and activity was substantial. I took some time to reflect on everything I saw. This post summarises the most important and noteworthy takeaways from my experience.
    </p>
  </div>

</div>

---

## Trends and Observations

The numbers defining NeurIPS 2025 are significant. There were 21,575 submissions, which represents a 61% increase over 2024. To put that in perspective, there were only about 9,400 submissions in 2020, meaning the field has more than doubled in five years. Despite the flood of papers, the acceptance rate remained steady at 24.5%, resulting in 5,290 accepted papers. To handle this volume, the organisers mobilised 20,518 reviewers. The peer-review process now requires a population larger than the seating capacity of Madison Square Garden.

The scale of the event was equally impressive in terms of attendance. There were roughly 29,000 total registrations. It is estimated that between 22,000 and 25,000 people attended in person in San Diego. This likely makes it the biggest academic gathering in history. The event was also diverse in its structure. In addition to the main algorithmic track, there were tracks for datasets and benchmarks, creative AI, position papers, and journals. This was all part of the main conference, which ran alongside workshops and other affinity events.

My personal observations matched these statistics. Walking through the hallways and listening to conversations, I frequently heard specific keywords. The most common terms were scale, reinforcement learning (RL), language models, world models, reasoning, and compute. These themes were reflected in the keynotes and poster sessions. The general trend holds that research based on Large Language Models (LLMs) dominated the conference and remains the prevailing direction for the AI community at large.

---

## Keynotes and Invited Talks

The conference featured six invited talks and one presentation for the Test of Time Award. I focused my attention on three specific sessions that offered distinct perspectives on the future of the field.

### The OAK Architecture: A Vision of Superintelligence from Experience
**Speaker:** Rich Sutton

Rich Sutton presented his perspective on the current state of AI research through the lens of the OAK architecture. His central thesis aligned closely with his well-known 'Bitter Lesson' hypothesis. He argued that to achieve true superintelligence, we must allow agents to learn entirely from experience. He cautioned against trying to explicitly induce user bias or expert knowledge into these systems.

The most surprising moment of the talk was his stance on continual learning. Sutton believes that robust and reliable continual learning is absolutely essential for superintelligence. While we do not possess this capability yet, he predicted that we might solve it within the next one to two years. This was a bold statement regarding a problem that remains an open question in the community. His belief suggests that we could arrive at superintelligence much sooner than many expect.

<div style="display: flex; gap: 12px; margin-bottom: 0.5em;">

  <div style="flex: 1;">
    <img src="/assets/IMG_0057.jpeg" alt="Richard Sutton keynote NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

  <div style="flex: 1;">
    <img src="/assets/IMG_0062.jpeg" alt="Opening keynote hall at NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

</div>

<p style="text-align: center; font-style: italic; margin-top: -0.2em;">
  The keynote sessions were organised in the giant exhibit hall (left). Opening keynote by Richard Sutton, delivered to a large audience at NeurIPS 2025.
</p>

### Are We Having the Wrong Nightmares About AI?
**Speaker:** Zeynep Tufekci

This talk moved away from technical architecture to address the societal impact of the technology. Tufekci argued that we are focusing on the wrong fears. Instead of worrying about AGI killing everyone or taking all jobs, she highlighted the realistic implications we are already seeing. She described the current generation of models as 'Artificial Good-Enough Intelligence'. These systems are capable enough to shatter the mechanisms of proof and authenticity that ground our social order. She implored AI researchers to put ethics and the impact of their work front and centre, rather than prioritising profits and high salaries.

### On the Science of “Alien Intelligences”
**Speaker:** Melanie Mitchell

Melanie Mitchell offered a compelling analogy for understanding Large Language Models (LLMs) and Vision Language Models (VLMs). She suggested we imagine an alien species has landed on Earth. This species appears intelligent, but the way it processes information is vastly different from human cognition. She argued that this is exactly how we should view AI. These models possess interesting capabilities, but we often draw false correlations regarding their understanding of the world. She recommended applying experimental protocols from developmental psychology to better evaluate these non-human cognitive systems.

### Other Invited Talks

I was unable to attend the full sessions for the remaining three keynotes, but they covered significant themes regarding the limits and theory of deep learning.

*   **The Art of (Artificial) Reasoning (Yejin Choi):** Choi discussed the limitations of brute-force scaling. She highlighted that state-of-the-art models still exhibit 'jagged intelligence' and argued for enhancing reasoning capabilities in smaller models to improve sustainability.
*   **From Benchmarks to Problems (Kyunghyun Cho):** Cho provided a retrospective on his work across distinct fields, from machine translation to drug discovery. He focused on the importance of problem-finding in AI and how seemingly unrelated technical problems often share close connections.
*   **Demystifying Depth (Andrew Saxe):** Saxe presented mathematical analyses of the nonlinear dynamics in deep neural networks. His talk offered theoretical insights into how learning algorithms and architectural choices interact to produce complex generalisation behaviours.

### General Inferences

The selection of these speakers signals a clear intention by the conference organisers. They placed the pursuit of autonomous, scaling-based intelligence (Sutton) directly alongside the need for caution and rigorous evaluation (Mitchell, Choi, Tufekci). It is evident that while the community is pushing for more powerful systems, there is a growing recognition that we must better understand the alien intelligence we are building before we integrate it further into society.

---

## Best Paper Awards

The Best Paper Award committees were tasked with selecting highly impactful papers from both the Main Track and the Datasets & Benchmark Track. Nominations were made by Program Chairs and approved by General Chairs, resulting in seven groundbreaking papers being recognised this year.

### Outstanding Paper Award
**Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free**
*Zihan Qiu et al.*

This paper addresses a fundamental component of modern AI: the attention mechanism. The authors conducted comprehensive experiments on over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models. Their central finding is that a simple modification, applying a head-specific sigmoid gate after the Scaled Dot-Product Attention, consistently improves performance. This "gated attention" enhances training stability, allows for larger learning rates, and mitigates the "attention sink" phenomenon. The selection committee praised the work for its extensive empirical evidence and for openly sharing results from industrial-scale experiments, a practice that has become increasingly rare.

### Outstanding Paper Award (Datasets & Benchmarks Track)
**Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)**
*Liwei Jiang et al.*

This paper tackles the issue of diversity in AI-generated content. The authors introduced "Infinity-Chat," a dataset of 26,000 open-ended user queries, to study how models respond to creative prompts. They discovered a "Hivemind" effect where models not only repeat themselves (intra-model repetition) but also produce strikingly similar outputs to one another (inter-model homogeneity). This work highlights a critical miscalibration between current reward models and diverse human preferences, raising concerns about the long-term homogenisation of human thought.

### Outstanding Paper Award
**1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities**
*Kevin Wang et al.*

While scaling has revolutionised language and vision, Reinforcement Learning (RL) has largely relied on shallow networks (2–5 layers). This paper challenges that convention by demonstrating that increasing depth to 1,024 layers can significantly boost performance in self-supervised RL. The authors show that deep networks allow agents to learn complex goal-reaching behaviours from scratch without external rewards. This finding suggests that RL can indeed scale efficiently with depth, unlocking new capabilities previously thought impossible without heavy supervision.

### Outstanding Paper Award
**Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training**
*Tony Bonnaire et al.*

This theoretical work investigates why diffusion models are able to generalise rather than simply memorising their training data. The authors identified two distinct timescales in training: an early phase for generalisation and a later phase where memorisation begins. Crucially, they found that the window for effective generalisation grows linearly with the size of the training set. This provides a rigorous mathematical explanation for the success of diffusion models, linking their performance to implicit dynamical regularisation.

### Test of Time Award
**Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks**
*Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun*

<div style="display: flex; align-items: center; gap: 20px; margin-bottom: 1.5em;">

  <!-- Image column (50%) -->
  <div style="flex: 0 0 50%;">
    <img src="/assets/IMG_0134.jpeg" alt="Kaiming He delivering the Test of Time keynote at NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

  <!-- Caption column (50%) -->
  <div style="flex: 0 0 50%; font-style: italic; text-align: left;">
    <p><strong>Kaiming He delivering the Test of Time keynote.</strong></p>
    <p>The Test of Time lecture highlighted a decade of influence from foundational work in computer vision and deep learning, marking how these early ideas continue to shape modern AI research. The acceptance speech was delivered by Kaiming He and was one of the most attended events of the conference.</p>
  </div>

</div>

He began with a general overview of the computer vision landscape before and after AlexNet, tracing the shift towards deep architectures that motivated this work. He then took a philosophical turn, describing his experience over the last decade as that of an explorer in uncharted waters. He likened research to searching for unknown lands, never knowing if discovery is around the corner or miles away. He remained remarkably humble, noting that newer innovations, such as the *Segment Anything Model* would eventually make works like *Faster R-CNN* obsolete. His talk served as a realistic and inspiring impetus for continued exploration in the field.

### Sejnowski-Hinton Prize
**Feedback Alignment**
*Timothy Lillicrap et al.*

The 2025 Sejnowski-Hinton Prize was awarded to the 2016 paper *Random synaptic feedback weights support error backpropagation for deep learning*. The authors discovered "feedback alignment," demonstrating that neural networks can learn effectively using fixed, random feedback weights rather than the exact symmetry required by backpropagation. This provided a biologically plausible solution to the "weight transport problem," showing how real neurons might follow loss gradients without non-local information.

---

## Poster Presentations and Oral Sessions

<div style="margin-bottom: 1em;">
  <img src="/assets/IMG_0112.jpeg" alt="Poster session at NeurIPS 2025" style="width: 100%; border-radius: 6px;">
</div>

<p style="text-align: center; font-style: italic; margin-top: -0.5em;">
  Lively and crowded poster sessions throughout the week.
</p>

The conference was structured around six massive poster sessions and a comprehensive series of oral presentations. While the oral sessions provided a platform for highlighting specific advancements, the poster hall was where the sheer scale of the community became evident.

### Oral Sessions
The oral sessions were diverse, covering the full spectrum of the field. The tracks included:
*   **Language Models:** Three separate sessions dedicated to the dominant architecture of the year.
*   **Generation & Simulation:** Covering generative AI and world-simulators.
*   **Multimodal:** Three sessions focusing on the intersection of vision, text, and audio.
*   **Deep Learning & Theory:** Multiple sessions diving into the mathematical foundations and architectural improvements.
*   **Reinforcement Learning & State-Space Models:** A strong showing for control systems and non-transformer architectures.
*   **Applications:** Spanning three sessions on real-world deployment.
*   **Special Tracks:** Including Position Papers, Neuroscience, Optimization, Algorithms, and Graph Neural Networks.

### The Poster Hall Experience
The posters were organised in a massive exhibition hall, segmented into distinct areas: Theory, Deep Learning, General Machine Learning, Applications, Computer Vision, and others. This zoning was essential; without it, navigating the thousands of papers would have been impossible. It allowed attendees to strategically decide where to spend their time based on their specific research interests.

Given the volume of work, I primarily focused my time on the Deep Learning, Applications, and Computer Vision sections. Even within these specific zones, the influence of Large Language Models (LLMs) was inescapable. Whether applied to general reasoning tasks or specific vertical applications, LLMs dominated the landscape.

**Key Themes Observed:**
*   **Agentic Workflows:** There was a significant number of papers moving beyond simple chatbots to autonomous agents capable of planning and executing multi-step workflows.
*   **Reasoning-Based Models:** A major focus was placed on improving the logical deduction capabilities of models, moving them from pattern matchers to reasoners.
*   **Multimodality:** Computer vision is no longer a siloed field; it is deeply integrated with language models, with numerous papers exploring how to ground language in visual reality.
*   **Architectures and Benchmarks:** Beyond new models, there was healthy introspection in the form of papers analysing the strengths and weaknesses of current paradigms and proposing new benchmarks to measure progress more accurately.

Ultimately, the poster sessions proved that despite the dominance of a few key trends, the field remains incredibly broad. Whether you were looking for pure mathematical theory or practical engineering applications, there was something for everyone.

---

## Expo and Sponsored Booths

The Expo Hall felt like a distinct world compared to the academic rigour of the poster sessions. It was dominated by some of the most valuable companies in the world, with massive booths from established AI giants and major tech conglomerates. Alongside the big tech players, there was a significant presence from quantitative trading firms and infrastructure providers, all keen to demonstrate their technical competencies.

<div style="display: flex; gap: 12px; margin-bottom: 0.5em;">

  <div style="flex: 1;">
    <img src="/assets/IMG_0068.jpeg" alt="Job board at the start of NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

  <div style="flex: 1;">
    <img src="/assets/IMG_0172.jpeg" alt="Job board by day three at NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

</div>

<p style="text-align: center; font-style: italic; margin-top: -0.2em;">
  The job board at the beginning of the conference (left) and by the end of day 3 (right).
</p>


Naturally, the hall was a magnet for attendees looking for open positions. The recruiting energy was palpable, with long queues of students and researchers waiting to discuss job opportunities and internships.

### Highlights and Demos
Beyond the recruiting frenzy, the technical demonstrations were the real draw.

*   **NVIDIA:** They showcased an impressive setup for robotic data collection using Virtual Reality (VR). The demo illustrated how human operators in VR can teleoperate robots to generate high-quality training data for complex manipulation tasks.
*   **Tesla:** This was my personal highlight of the Expo. They presented a driving simulation powered by a generative world model. I had the chance to try it myself, driving a vehicle inside a completely simulated, neural-generated environment. In discussions with the Tesla researchers, they explained that they are using this generative world model to collect data and train their self-driving policies in a closed loop. They noted that this approach is yielding remarkable results for their next generation of self-driving cars.

<div style="display: flex; gap: 12px; justify-content: center; align-items: flex-start; margin-bottom: 0.5em;">

  <!-- Landscape image (will be wider naturally) -->
  <img src="/assets/IMG_0064.jpeg"
       alt="Expo booth at NeurIPS 2025"
       style="height: 280px; width: auto; border-radius: 6px;">

  <!-- Portrait image (will be narrower naturally) -->
  <img src="/assets/IMG_0090.jpeg"
       alt="Expo demonstration at NeurIPS 2025"
       style="height: 280px; width: auto; border-radius: 6px;">

</div>

<p style="text-align: center; font-style: italic; margin-top: -0.2em;">
 A scene from this year’s Expo (left). Demo from Tesla, allowing people to drive a car in a purely virtual and generated environment (right).
</p>

---

## Workshops:

Days 6 and 7 were dedicated to workshops. I noticed a distinct drop in energy during these final two days; with fewer attendees remaining, the atmosphere felt somewhat anticlimactic compared to the bustle of the main conference. However, despite the lower attendance, the quality of the content and the effort put in by the organisers and presenters remained high.

<div style="display: flex; align-items: center; gap: 20px; margin-bottom: 1.5em;">

  <!-- Image column (50%) -->
  <div style="flex: 0 0 50%;">
    <img src="/assets/IMG_0342.jpeg" alt="Chelsea Finn speaking at the Embodied World Models for Decision Making workshop at NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

  <!-- Caption column (50%) -->
  <div style="flex: 0 0 50%; font-style: italic; text-align: left;">
    <p><strong>Chelsea Finn speaking at the “Embodied World Models for Decision Making” workshop.</strong></p>
    <p>The workshop sessions on days 6 and 7 brought a more intimate, research-focused atmosphere, with deep discussions on embodied agents, world models, and decision-making across interactive environments.</p>
  </div>

</div>

I focused on workshops that aligned closely with my specific research interests, hopping between sessions to catch key talks. My schedule included:

*   **Embodied World Models for Decision Making**
*   **The First Workshop on Efficient Reasoning**
*   **UniReps: Unifying Representations in Neural Models**
*   **Foundations of Reasoning in Language Models**
*   **Workshop on Scaling Environments for Agents**
*   **Workshop on Space in Vision, Language, and Embodied AI**
*   **LAW 2025: Bridging Language, Agent, and World Models for Reasoning and Planning**

Overall, these sessions followed a similar structure to the main conference but on a smaller, more intimate scale. They featured keynotes from prominent researchers in their respective niches, along with interesting oral talks, dedicated poster sessions, and their own best paper awards.

---

## General Observations: Venue and Vibe

Beyond the technical sessions, the venue itself played a significant role in the experience. Coming from the grey winter of Berlin, the warm, sunny weather in San Diego was a delightful change.

<div style="display: flex; gap: 12px; margin-bottom: 0.5em;">

  <div style="flex: 1;">
    <img src="/assets/IMG_0572.jpeg" alt="Gaslamp Quarter view from NeurIPS venue" style="width: 100%; border-radius: 6px;">
  </div>

  <div style="flex: 1;">
    <img src="/assets/IMG_0435.jpeg" alt="San Diego Bay and Coronado view from NeurIPS venue" style="width: 100%; border-radius: 6px;">
  </div>

</div>

<p style="text-align: center; font-style: italic; margin-top: -0.2em;">
  Views from outside the conference venue. Many participants could be seen socialising, taking a stroll, or relaxing here during breaks and evenings.
</p>

The San Diego Convention Center is perfectly situated. On one side lies the San Diego Bay, offering beautiful views, and on the other sits the historic Gaslamp Quarter. In the evenings, the area was teeming with conference attendees taking strolls along the water or heading into the city for dinner and drinks. It gave the event a lively, social atmosphere that extended well beyond the closing of the exhibition hall.

Inside the venue, the logistics were a mixed bag. A common topic of conversation was the food; while many people complained about the quality, I personally found it to be perfectly adequate. However, the sheer size of the convention centre was undeniable. Moving between the Expo Hall and the Keynote Hall was a trek that took at least five minutes, often longer during the main conference days. With over 20,000 people attempting to navigate the corridors simultaneously, bumping into others was an inevitability rather than a possibility.

---

## Closing Remarks

This was my first time attending NeurIPS. My previous experiences were primarily at computer vision conferences, so the sheer breadth of research areas here was striking. The diverse backgrounds of the attendees made for very interesting conversations that went beyond my usual technical bubble.

During my own poster session, I had the chance to connect with many experienced researchers and PhD students who are interested in a similar line of work. I found it remarkable that even at a conference of this magnitude, people still manage to filter through the noise. Attendees made a genuine effort to find the posters that aligned with their interests and took the time to have deep discussions with the authors.

Ultimately, the most valuable aspect of the conference was this networking. Meeting people face-to-face and having honest conversations offers value that simply reading papers online cannot replicate. I left San Diego feeling inspired by the community and I look forward to seeing how our field evolves before we meet again next year.
