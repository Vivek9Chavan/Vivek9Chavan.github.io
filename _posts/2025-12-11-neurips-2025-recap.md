---
layout: post
title: "NeurIPS 2025: My Takeaways”"
date: 2025-12-11
---

### TL;DR
*   **The Scale:** We hit a breaking point. **21,575 submissions** (+61% from last year) and over 20,000 reviewers. The conference is now the size of a small city.
*   **The Shift:** The "Chatbot" era is ending; the **"Reasoning & Agents"** era has begun. The focus has moved from *what models know* to *how they think*.
*   **The Vibe:** Optimism mixed with "Data Anxiety." Everyone is excited about System 2 thinking, but nervous about where the next trillion tokens will come from.

---

## Introduction: Sunshine and Silicon

I just got back from San Diego, and my brain is officially full.

Walking into the San Diego Convention Center for **NeurIPS 2025**, the first thing that hit me wasn't the sea of poster boards or the hum of thousands of laptops—it was the sheer density of human intelligence. If you threw a rock in the lobby (please don't), you’d likely hit three PhDs and a VC looking for the next OpenAI.

This year felt different. In 2023 and 2024, the air was thick with the initial hype of ChatGPT and the scramble to understand LLMs. This year, the hype has settled into something heavier: **Engineering Reality.** We aren't just marveling that the dog can talk anymore; we are trying to teach it to do calculus and book our flights reliably.

While the San Diego sun was bright outside, the real heat was in the Expo Hall, where the industry presence has grown so large it almost threatens to swallow the academic roots of the conference. Yet, despite the corporate flashiness, the heart of NeurIPS remains the poster sessions—where the real science happens, one awkward conversation at a time.

---

## The Opening Ceremony: "The Explosion Graph"

If you missed the opening remarks, you missed a collective gasp from the audience. The Program Chairs put up a slide that didn't just show growth; it showed a vertical wall.

Here is the breakdown of the numbers that defined NeurIPS 2025:

*   **21,575 Submissions:** This is the headline number. It represents a **61% increase** over 2024. To put that in perspective, in 2020, there were only ~9,400 submissions. We have more than doubled the field in five years.
*   **24.5% Acceptance Rate:** Despite the flood of papers, the bar remained steady. 5,290 papers were accepted.
*   **The "Army" of Reviewers:** This was the most staggering stat on the screen. To handle 21k papers, the organizers had to mobilize **20,518 reviewers**. The peer-review process now requires a population larger than the seating capacity of Madison Square Garden.

### What are people working on?
Another slide broke down the submissions by topic, and the trend is undeniable:
1.  **LLMs & Foundation Models:** Still the king, by a landslide.
2.  **Datasets & Benchmarks:** This track **doubled** in size (1,995 submissions). This confirms the industry's shift toward "Data-Centric AI"—we realized that better models need better diets, not just bigger parameters.
3.  **Generative Models:** Specifically video generation and multimodal reasoning.

As I sat there watching these numbers, the theme for the week became clear: **AI is no longer a niche research field; it is a global industrial complex.** And we are all just trying to keep up with the graph.

---

## The Big Themes: Beyond Next-Token Prediction

If 2023 was the year of "Wow, it can write poetry," and 2024 was the year of "How do we make it smaller?", 2025 is undeniably the year of **"How do we make it think?"**

Walking the poster floor and eavesdropping on conversations in the coffee lines (which were, by the way, agonizingly long), three distinct themes emerged that dominated the discourse.

### 1. The Shift to "System 2" Reasoning
The most palpable shift this year is the move away from pure "System 1" intuition (fast, pattern-matching responses) toward "System 2" reasoning (slow, deliberate, logical steps).

For the last two years, we’ve been trying to prompt-engineer our way into better reasoning ("Let's think step by step"). Now, that logic is moving **inside the architecture**. I saw dozens of papers focusing on **inference-time compute**—essentially allowing the model to "ponder" a problem for seconds or minutes before outputting a single token.

The consensus is clear: we are hitting diminishing returns on simply making models bigger. The next frontier isn't *more parameters*; it's *more time to think*.

### 2. Agents: From "Toys" to "Reliable Planners"
"Agents" was the buzzword of 2024, but let’s be honest—most of them were brittle toys that got stuck in loops.

At NeurIPS 2025, the tone changed. The research has pivoted from "Look, it can use a tool!" to **"How do we make it stop hallucinating a plan?"**

I noticed a massive uptick in papers regarding **Long-Horizon Planning** and **Self-Correction**. The most impressive demos weren't the ones doing flashy creative work, but the ones doing boring administrative tasks *reliably*. The community is finally tackling the "last mile" problem of agents: error recovery. If an agent fails step 3 of 10, it shouldn't restart; it should realize its mistake and pivot.

### 3. The "Data Wall" and the Synthetic Solution
With the Datasets & Benchmarks track doubling in size, it’s obvious that the industry is anxious about the "Data Wall"—the reality that we have scraped nearly every high-quality token on the public internet.

The solution? **Curated Synthetic Data.**

Two years ago, "synthetic data" was a dirty word, associated with model collapse and artifacts. This year, it’s the hero. I saw multiple posters demonstrating that models trained on *highly curated* synthetic data (generated by larger "teacher" models) are actually outperforming models trained on messy web data. We are moving from the "Hunter-Gatherer" phase of data (scraping everything) to the "Farming" phase (growing our own data).

---

## Keynote Highlights

With six keynotes and the Test of Time award, it’s impossible to cover everything without writing a novel. However, two talks stood out to me as defining moments of the conference.

<div style="display: flex; align-items: flex-start; margin-bottom: 1.5em; gap: 20px;">

  <!-- Image column (50%) -->
  <div style="flex: 0 0 40%;">
    <img src="/assets/IMG_0057.jpeg" alt="Richard Sutton keynote at NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

  <!-- Text column (50%) -->
  <div style="flex: 1;">
    <p><strong>Opening keynote by Richard Sutton, delivered to a packed auditorium at NeurIPS 2025.</strong></p>

    <p>Richard Sutton opened NeurIPS 2025 with a keynote that set both the intellectual tone and emotional atmosphere of the conference. The room was completely full—thousands of researchers. Sutton used the keynote to return to first principles: the nature of prediction, the essence of reinforcement learning, and the long-term perspective required to build agents that continually learn from interaction.</p>
  </div>

</div>

### Rich Sutton: The Bitter Lesson 2.0
Rich Sutton took the stage, and the room was dead silent. Known for his famous essay "The Bitter Lesson" (which argued that general methods that leverage computation eventually beat clever human design), everyone expected a victory lap.

Instead, he doubled down. He argued that we are still trying to hand-engineer too much "reasoning" into our systems. His thesis for 2025? **"Let the environment dictate the reasoning."** He pushed for a return to Reinforcement Learning (RL) as the primary driver for intelligence, suggesting that the "System 2" reasoning we are all excited about should emerge from RL training, not be hard-coded into architectures. It was controversial, provocative, and exactly what a keynote should be.

### Yejin Choi: The Limits of the Impossible
On the other end of the spectrum, Yejin Choi gave a grounding reality check. While Sutton spoke of infinite scaling, Choi spoke of **Common Sense**.

She highlighted the "Paradox of Knowledge": models can pass the Bar Exam but still struggle with basic physical causality that a three-year-old understands. Her slides were filled with hilarious (and worrying) examples of state-of-the-art models failing at trivial tasks. Her main takeaway? **"Reasoning is not just logic; it is world-modeling."** Until models have a coherent internal model of physics and causality, they will remain brilliant but brittle savants.

---

## Best Papers: Efficiency Over Scale

Every year, the "Best Paper" awards signal where the community’s values are shifting. If you expected the award to go to the largest model with the most zeros in its parameter count, you would be wrong.

### The Outstanding Paper Award
The top honor went to **Qiu et al.** for their work on **Dynamic Gating Mechanisms in Transformers**.

*   **The Gist:** Instead of activating the entire neural network for every single token, their method dynamically selects which "experts" or sub-networks are needed on the fly.
*   **Why it matters:** This is a direct response to the energy crisis in AI. We are running out of GPUs and electricity. This paper proves that the community is now prioritizing **inference efficiency** just as much as raw capability. It’s not sexy, but it’s the plumbing that will make AI sustainable.

### Test of Time Award: Faster R-CNN (2015)
The Test of Time award (given to a paper from 10 years ago) went to **Faster R-CNN** by Ren et al.
Seeing this on the big screen was a massive nostalgia trip. In 2015, drawing a bounding box around a cat was the cutting edge. Today, we are generating 3D video of cats playing pianos. It was a humbling reminder of how fast a decade moves in this field.

---

## Poster Presentations: The Real Conference

If the keynotes are the "concert," the poster sessions are the "mosh pit."

This is where the real work of NeurIPS happens. The setup was the usual endless maze of boards, but the crowd dynamics were fascinating this year.

<div style="display: flex; align-items: flex-start; margin-bottom: 1.5em; gap: 20px;">

  <!-- Image column (50%) -->
  <div style="flex: 0 0 50%;">
    <img src="/assets/IMG_0112.jpeg" alt="Poster session with lively discussions at NeurIPS 2025" style="width: 100%; border-radius: 6px;">
  </div>

  <!-- Text column (50%) -->
  <div style="flex: 1;">
    <p><strong>Lively discussions during the poster sessions.</strong></p>

    <p>The poster sessions at NeurIPS 2025 were among the most dynamic parts of the conference. Crowds formed quickly around anything related to agentic systems, multimodal learning, data efficiency, and alignment—reflecting where the community’s curiosity is moving. It was often impossible to walk in a straight line through the hall, simply because each poster had its own circle of conversation, debate, and spontaneous collaboration. What stood out most was how deeply technical these conversations were. Researchers were sketching architectures on notepads, pulling up GitHub repos on laptops, and discussing failure cases and practical insights rather than simply presenting polished results.</p>
  </div>

</div>

### The "Hype Circles"
You could instantly tell which papers were trending on Twitter (X) by the crowd density.
*   **The "Reasoning" Crowd:** Any poster with "System 2," "Chain of Thought," or "O1-like" in the title had a crowd five people deep. I tried to get close to a poster about **"Recursive Self-Correction in Agents,"** but I honestly couldn't hear the presenter over the questions.
*   **The "Theory" Deserts:** In contrast, the pure theory sections (optimization bounds, convex analysis) were quieter. However, this is where I had my best conversation of the week. I spent twenty minutes talking to a PhD student about the **theoretical limits of in-context learning**. No hype, no buzzwords, just math. It was a breath of fresh air.

### My Personal Favorite Poster
I stumbled upon a paper regarding **"Small Language Models as Verifiers."**
The premise was simple: you don't need a massive model to *generate* the answer, but you can use a massive model to *check* the work of a smaller model. It felt like a practical, immediate solution to deployment costs that I can actually use in my work next week.

---

## The Expo: Where the Money Is

Walking from the poster hall to the Expo Hall felt like crossing a border between two different countries.

*   **The Poster Hall:** Academic, messy, poorly dressed, fueled by bad coffee.
*   **The Expo Hall:** Corporate, polished, aggressive, fueled by venture capital.

The Expo is where the sponsors flex. The booths this year were larger than ever, with NVIDIA, Google DeepMind, and OpenAI taking up massive real estate.

### What I Noticed:
1.  **The "Full Stack" Pivot:** In previous years, companies were selling "Model APIs." This year, everyone was selling **"Enterprise Platforms."** The pitch has shifted from "Use our LLM" to "Build your entire Agentic Workflow on our secure infrastructure."
2.  **Robotics is Back:** There were robot dogs, humanoid arms folding laundry, and drones everywhere. With the rise of multimodal models (vision + language + action), robotics has finally found the "brain" it was missing for the last decade.
3.  **Recruiting Wars:** The swag was better this year, which usually means the recruiting is desperate. I saw companies handing out high-end mechanical keyboards just to get a resume scan.

Despite the flashiness, I found the Expo a bit exhausting. It’s a stark reminder that while the researchers upstairs are trying to solve AGI, the downstairs crowd is trying to figure out how to sell it to the Fortune 500.
